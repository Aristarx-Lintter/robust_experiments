{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc6fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/detector/')\n",
    "os.chdir('/detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5f0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.backbone import get_model\n",
    "from timm import create_model\n",
    "\n",
    "model = create_model('efficientnet_b0', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d63907",
   "metadata": {},
   "source": [
    "# GPU device benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8008b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_FP32_MODEL_NAME = 'weights/trt_model_fp32.ts'\n",
    "TRT_FP16_MODEL_NAME = 'weights/trt_model_fp16.ts'\n",
    "DEVICE = 'cuda:0'\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88ac17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from src.transforms import torch_preprocessing\n",
    "\n",
    "image = cv2.imread('datasets/birdview_vehicles/train/1.jpg')[..., ::-1]\n",
    "torch_input = torch_preprocessing(image, image_size=(224, 224)).to(DEVICE)\n",
    "torch_input = torch.cat([torch_input] * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e43f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    torch_out = model(torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffd286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "trt_model_fp32 = torch_tensorrt.compile(\n",
    "    model,\n",
    "    inputs = [torch_tensorrt.Input((BATCH_SIZE, 3, 224, 224))],\n",
    "    enabled_precisions = torch.float32,\n",
    "    workspace_size = 1 << 30, # 1 гибибайт\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba6afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trt_model_fp32, TRT_FP32_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea9ed7",
   "metadata": {},
   "source": [
    "## TRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b710fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9216044959994178"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "trt_model_fp32 = torch.jit.load(TRT_FP32_MODEL_NAME)\n",
    "\n",
    "# st = time\n",
    "timeit.timeit(lambda: trt_model_fp32(torch_input).cpu().numpy(), number=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8d505",
   "metadata": {},
   "source": [
    "## torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "118ff702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.951143038000737"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(lambda: model(torch_input).cpu().detach().numpy(), number=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98134424",
   "metadata": {},
   "source": [
    "# CPU device benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41958bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "ONNX_MODEL_NAME = 'weights/onnx_model.onnx'\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224, device='cpu')\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    ONNX_MODEL_NAME,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "#     dynamic_axes = {'input': [0], 'output': [0]}, # динамический батч, но можно и статический\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbeefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "OPENVINO_FOLDER = 'weights/openvino_model'\n",
    "OPENVINO_MODEL_NAME = f'{OPENVINO_FOLDER}/onnx_model.xml'\n",
    "OPENVINO_WEIGHTS_NAME = f'{OPENVINO_FOLDER}/onnx_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "164b91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release. Please use OpenVINO Model Converter (OVC). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /detector/weights/openvino_model/onnx_model.xml\n",
      "[ SUCCESS ] BIN file: /detector/weights/openvino_model/onnx_model.bin\n"
     ]
    }
   ],
   "source": [
    "! mo --input_model {ONNX_MODEL_NAME} --output_dir {OPENVINO_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44092c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "ovmodel = ie.read_model(model=OPENVINO_MODEL_NAME, weights=OPENVINO_WEIGHTS_NAME)\n",
    "compiled_model = ie.compile_model(model=ovmodel)\n",
    "output_layer = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33e0fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.transforms import openvino_preprocessing\n",
    "\n",
    "openvino_input = np.concatenate([openvino_preprocessing(image, image_size=(224, 224))] * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f7703",
   "metadata": {},
   "source": [
    "## onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "353dbdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.795465109998986"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(lambda: compiled_model([openvino_input])[output_layer], number=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46425ea",
   "metadata": {},
   "source": [
    "## torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea5ddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.820241415000055"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(lambda: model(torch_input.cpu()).detach().numpy(), number=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
